{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOI0kY2VXtRLjjaSMhASBxM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaiDheerajPeketi/Major-Project-CSC06-Alt/blob/main/Major_Project_Reboot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtEzQJ_a_y6T"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
        "import joblib\n",
        "import os\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create directories for saving models and results\n",
        "os.makedirs('models', exist_ok=True)\n",
        "os.makedirs('results', exist_ok=True)"
      ],
      "metadata": {
        "id": "X7_yNSaMAMoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "df = pd.read_csv('/content/drive/MyDrive/Dataset/raw_combined_data.csv')"
      ],
      "metadata": {
        "id": "wrYpSsOSARja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Exploration\n",
        "print(\"Data shape:\", df.shape)\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nData types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "print(\"\\nMissing values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"\\nDescriptive statistics:\")\n",
        "print(df.describe())"
      ],
      "metadata": {
        "id": "vxiIq4TdARlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert timestamp to datetime\n",
        "if 'Timestamp' in df.columns and not df['Timestamp'].isna().all():\n",
        "    df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce')"
      ],
      "metadata": {
        "id": "5GAb49piARn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to fill missing values\n",
        "def fill_missing_values(df):\n",
        "    # For SpO2 columns - fill with median as it's a vital sign with normal range\n",
        "    spo2_cols = [col for col in df.columns if 'SpO2' in col]\n",
        "    for col in spo2_cols:\n",
        "        if df[col].isna().sum() > 0 and df[col].notna().sum() > 0:\n",
        "            df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "    # For heart rate columns - fill with median\n",
        "    hr_cols = [col for col in df.columns if '_HR' in col]\n",
        "    for col in hr_cols:\n",
        "        if df[col].isna().sum() > 0 and df[col].notna().sum() > 0:\n",
        "            df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "    # For PI (Perfusion Index) - fill with median\n",
        "    pi_cols = [col for col in df.columns if '_PI' in col]\n",
        "    for col in pi_cols:\n",
        "        if df[col].isna().sum() > 0 and df[col].notna().sum() > 0:\n",
        "            df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "    # For ETCO2, ETO2, ScalcO2, RR - fill with median\n",
        "    other_cols = ['ETCO2', 'ETO2', 'ScalcO2', 'RR']\n",
        "    for col in other_cols:\n",
        "        if col in df.columns and df[col].isna().sum() > 0 and df[col].notna().sum() > 0:\n",
        "            df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "mQ1tefZSARp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to clean extreme values\n",
        "def remove_extreme_values(df):\n",
        "    # Set limits for physiologically plausible values\n",
        "    limits = {\n",
        "        'SpO2': (60, 100),  # SpO2 range from 60% to 100%\n",
        "        'HR': (30, 200),  # Heart rate from 30 to 200 bpm\n",
        "        'PI': (0, 20),  # PI normally between 0 and 20\n",
        "        'ETCO2': (0, 100),  # End-tidal CO2 normally 35-45 mmHg but allowing wider range\n",
        "        'ETO2': (0, 100),  # End-tidal O2 percentage\n",
        "        'ScalcO2': (0, 100),  # Calculated O2 percentage\n",
        "        'RR': (4, 40)  # Respiratory rate from 4 to 40 breaths per minute\n",
        "    }\n",
        "\n",
        "    for col_type, (min_val, max_val) in limits.items():\n",
        "        cols = [col for col in df.columns if col_type in col]\n",
        "        for col in cols:\n",
        "            if col in df.columns:\n",
        "                # Replace values outside the limits with NaN\n",
        "                df[col] = df[col].apply(lambda x: x if pd.isna(x) or (min_val <= x <= max_val) else np.nan)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "KJJ06GkaARsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply cleaning functions\n",
        "df = remove_extreme_values(df)\n",
        "df = fill_missing_values(df)"
      ],
      "metadata": {
        "id": "lpUIspsmARu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns with too many missing values and check results\n",
        "threshold = 0.8 * len(df)\n",
        "df = df.dropna(axis=1, thresh=threshold)\n",
        "\n",
        "# Drop rows where all feature values are missing\n",
        "df = df.dropna(how='all')\n",
        "\n",
        "print(\"\\nData shape after cleaning:\", df.shape)\n",
        "print(\"\\nMissing values after cleaning:\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "k5ofw_yGARxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to create device-specific features\n",
        "def create_device_features(df):\n",
        "    # Identify device columns\n",
        "    devices = set([col.split('_')[0] for col in df.columns if '_' in col])\n",
        "\n",
        "    for device in devices:\n",
        "        device_cols = [col for col in df.columns if device in col]\n",
        "\n",
        "        # Only process if we have enough columns for this device\n",
        "        if len(device_cols) >= 2:\n",
        "            # Calculate mean and std of SpO2 for this device if available\n",
        "            spo2_col = [col for col in device_cols if 'SpO2' in col]\n",
        "            if spo2_col and df[spo2_col[0]].notna().sum() > 0:\n",
        "                df[f'{device}_SpO2_rolling_mean'] = df[spo2_col[0]].rolling(window=5, min_periods=1).mean()\n",
        "                df[f'{device}_SpO2_rolling_std'] = df[spo2_col[0]].rolling(window=5, min_periods=1).std().fillna(0)\n",
        "\n",
        "            # Calculate mean and std of HR for this device if available\n",
        "            hr_col = [col for col in device_cols if 'HR' in col]\n",
        "            if hr_col and df[hr_col[0]].notna().sum() > 0:\n",
        "                df[f'{device}_HR_rolling_mean'] = df[hr_col[0]].rolling(window=5, min_periods=1).mean()\n",
        "                df[f'{device}_HR_rolling_std'] = df[hr_col[0]].rolling(window=5, min_periods=1).std().fillna(0)\n",
        "\n",
        "            # If we have both SpO2 and HR, create a ratio feature\n",
        "            if spo2_col and hr_col and df[spo2_col[0]].notna().sum() > 0 and df[hr_col[0]].notna().sum() > 0:\n",
        "                df[f'{device}_SpO2_HR_ratio'] = df[spo2_col[0]] / df[hr_col[0]]\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "pieJFj6JAoMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to create temporal features\n",
        "def create_temporal_features(df):\n",
        "    if 'Timestamp' in df.columns and pd.api.types.is_datetime64_dtype(df['Timestamp']):\n",
        "        df['hour'] = df['Timestamp'].dt.hour\n",
        "        df['minute'] = df['Timestamp'].dt.minute\n",
        "        df['second'] = df['Timestamp'].dt.second\n",
        "\n",
        "        # Create time-based features for physiological patterns\n",
        "        df['time_of_day'] = df['hour'].apply(lambda x:\n",
        "                                             'night' if 0 <= x < 6 else\n",
        "                                             'morning' if 6 <= x < 12 else\n",
        "                                             'afternoon' if 12 <= x < 18 else\n",
        "                                             'evening')\n",
        "\n",
        "        # One-hot encode time of day\n",
        "        df = pd.get_dummies(df, columns=['time_of_day'], prefix='time')\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "4GiKTcxrAoOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply feature engineering functions\n",
        "try:\n",
        "    df = create_device_features(df)\n",
        "except Exception as e:\n",
        "    print(f\"Warning in create_device_features: {e}\")\n",
        "\n",
        "try:\n",
        "    df = create_temporal_features(df)\n",
        "except Exception as e:\n",
        "    print(f\"Warning in create_temporal_features: {e}\")"
      ],
      "metadata": {
        "id": "98hJeJLhAoRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create cross-device consistency features\n",
        "devices_spo2 = [col for col in df.columns if\n",
        "                'SpO2' in col and not col.endswith('_rolling_mean') and not col.endswith('_rolling_std')]\n",
        "if len(devices_spo2) >= 2:\n",
        "    # Calculate mean and std across devices\n",
        "    df['cross_device_SpO2_mean'] = df[devices_spo2].mean(axis=1)\n",
        "    df['cross_device_SpO2_std'] = df[devices_spo2].std(axis=1, ddof=0)\n",
        "\n",
        "    # Calculate device deviation from mean\n",
        "    for col in devices_spo2:\n",
        "        df[f'{col}_dev_from_mean'] = df[col] - df['cross_device_SpO2_mean']\n",
        "\n",
        "# Similarly for heart rate\n",
        "devices_hr = [col for col in df.columns if\n",
        "              '_HR' in col and not col.endswith('_rolling_mean') and not col.endswith('_rolling_std')]\n",
        "if len(devices_hr) >= 2:\n",
        "    df['cross_device_HR_mean'] = df[devices_hr].mean(axis=1)\n",
        "    df['cross_device_HR_std'] = df[devices_hr].std(axis=1, ddof=0)\n",
        "\n",
        "    for col in devices_hr:\n",
        "        df[f'{col}_dev_from_mean'] = df[col] - df['cross_device_HR_mean']\n",
        "\n",
        "print(\"\\nFeatures after engineering:\")\n",
        "print(df.columns.tolist())\n",
        "print(\"\\nData shape after feature engineering:\", df.shape)"
      ],
      "metadata": {
        "id": "wdMZsBCUAoT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preparation for modeling\n",
        "# Drop non-feature columns\n",
        "non_feature_cols = ['Timestamp', 'Sample']\n",
        "feature_cols = [col for col in df.columns if col not in non_feature_cols]\n",
        "\n",
        "# Handle remaining NaN values for modeling\n",
        "X = df[feature_cols].fillna(df[feature_cols].median())"
      ],
      "metadata": {
        "id": "SQ1-7usHAoWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to normalize data\n",
        "def normalize_data(X, method='standard'):\n",
        "    if method == 'standard':\n",
        "        scaler = StandardScaler()\n",
        "    elif method == 'robust':\n",
        "        scaler = RobustScaler()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid normalization method\")\n",
        "\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    return X_scaled, scaler\n"
      ],
      "metadata": {
        "id": "VEDrpYPnAoZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply normalization methods\n",
        "X_standard, standard_scaler = normalize_data(X, method='standard')\n",
        "X_robust, robust_scaler = normalize_data(X, method='robust')"
      ],
      "metadata": {
        "id": "SSF86oyuAobk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the scalers\n",
        "joblib.dump(standard_scaler, 'models/standard_scaler.pkl')\n",
        "joblib.dump(robust_scaler, 'models/robust_scaler.pkl')"
      ],
      "metadata": {
        "id": "sU0pb_QMARzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dimension reduction for visualization\n",
        "# Apply PCA for visualization\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_standard)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.5)\n",
        "plt.title('PCA of Patient Data')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.savefig('results/pca_visualization.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "_bhwEscBAR2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to evaluate and save models\n",
        "def evaluate_and_save_model(name, model, X, X_pca=None):\n",
        "    # Make predictions\n",
        "    if hasattr(model, 'fit_predict'):\n",
        "        y_pred = model.fit_predict(X)\n",
        "    else:\n",
        "        model.fit(X)\n",
        "        y_pred = model.predict(X)\n",
        "\n",
        "    # Convert predictions to outlier labels (assuming -1 or lower scores are outliers)\n",
        "    if name == 'LOF' or name == 'IsolationForest':\n",
        "        outliers = y_pred == -1\n",
        "    else:  # For clustering-based methods\n",
        "        outliers = y_pred == -1  # Adjust based on specific algorithm output\n",
        "\n",
        "    # Calculate percentage of outliers\n",
        "    outlier_percentage = 100 * outliers.sum() / len(outliers)\n",
        "\n",
        "    # Save the model\n",
        "    model_filename = f'models/{name}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.pkl'\n",
        "    joblib.dump(model, model_filename)\n",
        "\n",
        "    # If we have PCA data, visualize the results\n",
        "    if X_pca is not None:\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.scatter(X_pca[~outliers, 0], X_pca[~outliers, 1], c='blue', alpha=0.5, label='Inliers')\n",
        "        plt.scatter(X_pca[outliers, 0], X_pca[outliers, 1], c='red', alpha=0.5, label='Outliers')\n",
        "        plt.title(f'Outlier Detection using {name} (Outliers: {outlier_percentage:.2f}%)')\n",
        "        plt.xlabel('Principal Component 1')\n",
        "        plt.ylabel('Principal Component 2')\n",
        "        plt.legend()\n",
        "        plt.savefig(f'results/{name}_outliers.png')\n",
        "        plt.close()\n",
        "\n",
        "    # Try to calculate silhouette score if applicable (for clustering-based methods)\n",
        "    sil_score = None\n",
        "    if name in ['KMeans', 'DBSCAN'] and len(np.unique(y_pred)) > 1:\n",
        "        try:\n",
        "            sil_score = silhouette_score(X, y_pred)\n",
        "        except:\n",
        "            sil_score = \"Not applicable\"\n",
        "\n",
        "    # Calculate some basic statistics on the outliers\n",
        "    outlier_indices = np.where(outliers)[0]\n",
        "\n",
        "    results = {\n",
        "        'model_name': name,\n",
        "        'outlier_percentage': outlier_percentage,\n",
        "        'silhouette_score': sil_score,\n",
        "        'outlier_indices': outlier_indices,\n",
        "        'model_path': model_filename\n",
        "    }\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "4TiHDfxyAR4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize dictionary to store results\n",
        "model_results = {}"
      ],
      "metadata": {
        "id": "J-gBQUSlB_Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K-Means Clustering\n",
        "print(\"\\nTraining K-Means clustering model...\")\n",
        "best_kmeans = None\n",
        "best_score = -1\n",
        "best_k = 2\n",
        "\n",
        "for k in range(2, 11):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X_standard)\n",
        "    labels = kmeans.labels_\n",
        "\n",
        "    # Try to calculate silhouette score\n",
        "    try:\n",
        "        score = silhouette_score(X_standard, labels)\n",
        "        print(f\"KMeans with {k} clusters - Silhouette Score: {score:.4f}\")\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_k = k\n",
        "            best_kmeans = kmeans\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "if best_kmeans:\n",
        "    # Add a cluster for outliers (could be the smallest or most distant cluster)\n",
        "    labels = best_kmeans.labels_\n",
        "    cluster_sizes = np.bincount(labels)\n",
        "    smallest_cluster = np.argmin(cluster_sizes)\n",
        "\n",
        "    # Mark the smallest cluster as outliers (-1)\n",
        "    kmeans_outlier_labels = np.array(labels, copy=True)\n",
        "    kmeans_outlier_labels[labels == smallest_cluster] = -1\n",
        "    kmeans_outlier_labels[labels != smallest_cluster] = 1\n",
        "\n",
        "    # Evaluate and save\n",
        "    kmeans_results = evaluate_and_save_model('KMeans', best_kmeans, X_standard, X_pca)\n",
        "    kmeans_results['best_k'] = best_k\n",
        "    kmeans_results['silhouette_score'] = best_score\n",
        "    model_results['KMeans'] = kmeans_results"
      ],
      "metadata": {
        "id": "EB33ZXy7B_QL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DBSCAN Clustering\n",
        "print(\"\\nTraining DBSCAN model...\")\n",
        "best_dbscan = None\n",
        "best_score = -1\n",
        "best_eps = 0.5\n",
        "best_min_samples = 5\n",
        "\n",
        "param_grid = {\n",
        "    'eps': [0.3, 0.5, 0.7, 1.0],\n",
        "    'min_samples': [5, 10, 15, 20]\n",
        "}\n",
        "\n",
        "for eps in param_grid['eps']:\n",
        "    for min_samples in param_grid['min_samples']:\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        labels = dbscan.fit_predict(X_standard)\n",
        "\n",
        "        # Skip if all points are classified as noise (-1)\n",
        "        if np.all(labels == -1) or len(np.unique(labels)) == 1:\n",
        "            continue\n",
        "\n",
        "        # Try to calculate silhouette score\n",
        "        try:\n",
        "            score = silhouette_score(X_standard, labels)\n",
        "            print(f\"DBSCAN with eps={eps}, min_samples={min_samples} - Silhouette Score: {score:.4f}\")\n",
        "\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_eps = eps\n",
        "                best_min_samples = min_samples\n",
        "                best_dbscan = dbscan\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "if best_dbscan:\n",
        "    # Evaluate and save\n",
        "    dbscan_results = evaluate_and_save_model('DBSCAN', best_dbscan, X_standard, X_pca)\n",
        "    dbscan_results['best_eps'] = best_eps\n",
        "    dbscan_results['best_min_samples'] = best_min_samples\n",
        "    dbscan_results['silhouette_score'] = best_score\n",
        "    model_results['DBSCAN'] = dbscan_results"
      ],
      "metadata": {
        "id": "6Aj4Y2WkB_Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Isolation Forest\n",
        "print(\"\\nTraining Isolation Forest model...\")\n",
        "best_iso_forest = None\n",
        "best_auc = -1\n",
        "best_contamination = 0.1\n",
        "\n",
        "for contamination in [0.01, 0.05, 0.1, 0.15]:\n",
        "    iso_forest = IsolationForest(\n",
        "        n_estimators=100,\n",
        "        max_samples='auto',\n",
        "        contamination=contamination,\n",
        "        random_state=42\n",
        "    )\n",
        "    iso_forest.fit(X_standard)\n",
        "    model_results[f'IsolationForest_{contamination}'] = evaluate_and_save_model(\n",
        "        f'IsolationForest_{contamination}',\n",
        "        iso_forest,\n",
        "        X_standard,\n",
        "        X_pca\n",
        "    )\n"
      ],
      "metadata": {
        "id": "7zqlnk4hB_gM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Local Outlier Factor\n",
        "print(\"\\nTraining Local Outlier Factor model...\")\n",
        "for n_neighbors in [5, 10, 20]:\n",
        "    lof = LocalOutlierFactor(\n",
        "        n_neighbors=n_neighbors,\n",
        "        contamination=0.1\n",
        "    )\n",
        "    model_results[f'LOF_{n_neighbors}'] = evaluate_and_save_model(\n",
        "        f'LOF_{n_neighbors}',\n",
        "        lof,\n",
        "        X_standard,\n",
        "        X_pca\n",
        "    )"
      ],
      "metadata": {
        "id": "VCTj-8AeAR7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Feature Importance\n",
        "# For IsolationForest, we can get feature importance\n",
        "if 'IsolationForest_0.1' in model_results:\n",
        "    iso_forest_model = joblib.load(model_results['IsolationForest_0.1']['model_path'])\n",
        "\n",
        "    if hasattr(iso_forest_model, 'feature_importances_'):\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'Feature': feature_cols,\n",
        "            'Importance': iso_forest_model.feature_importances_\n",
        "        })\n",
        "\n",
        "        feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.barplot(x='Importance', y='Feature', data=feature_importance.head(20))\n",
        "        plt.title('Top 20 Features by Importance')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('results/feature_importance.png')\n",
        "        plt.close()"
      ],
      "metadata": {
        "id": "pjjlskChAR9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Summary and Reporting\n",
        "# Print model evaluation summary\n",
        "print(\"\\n=== MODEL EVALUATION SUMMARY ===\")\n",
        "for model_name, results in model_results.items():\n",
        "    print(f\"\\nModel: {model_name}\")\n",
        "    print(f\"Outlier Percentage: {results['outlier_percentage']:.2f}%\")\n",
        "    if results['silhouette_score'] is not None:\n",
        "        print(f\"Silhouette Score: {results['silhouette_score']}\")\n",
        "    print(f\"Model saved to: {results['model_path']}\")"
      ],
      "metadata": {
        "id": "O9XgF17iCZjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save summary to file\n",
        "with open('results/model_summary.txt', 'w') as f:\n",
        "    f.write(\"=== MODEL EVALUATION SUMMARY ===\\n\")\n",
        "    for model_name, results in model_results.items():\n",
        "        f.write(f\"\\nModel: {model_name}\\n\")\n",
        "        f.write(f\"Outlier Percentage: {results['outlier_percentage']:.2f}%\\n\")\n",
        "        if results['silhouette_score'] is not None:\n",
        "            f.write(f\"Silhouette Score: {results['silhouette_score']}\\n\")\n",
        "        f.write(f\"Model saved to: {results['model_path']}\\n\")\n",
        "        f.write(f\"Number of outliers detected: {len(results['outlier_indices'])}\\n\")\n",
        "\n",
        "print(\"\\nAnalysis complete! Results saved to 'results' directory.\")\n",
        "print(\"Models saved to 'models' directory.\")"
      ],
      "metadata": {
        "id": "snXUhLC3CZl2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}